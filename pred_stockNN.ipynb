{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e899e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a19c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6502b569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|Index|      Date|      Open|      High|       Low|     Close| Adj Close|Volume|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|  NYA|1965-12-31|528.690002|528.690002|528.690002|528.690002|528.690002|     0|\n",
      "|  NYA|1966-01-03|527.210022|527.210022|527.210022|527.210022|527.210022|     0|\n",
      "|  NYA|1966-01-04|527.840027|527.840027|527.840027|527.840027|527.840027|     0|\n",
      "|  NYA|1966-01-05|531.119995|531.119995|531.119995|531.119995|531.119995|     0|\n",
      "|  NYA|1966-01-06|532.070007|532.070007|532.070007|532.070007|532.070007|     0|\n",
      "|  NYA|1966-01-07|532.599976|532.599976|532.599976|532.599976|532.599976|     0|\n",
      "|  NYA|1966-01-10|533.869995|533.869995|533.869995|533.869995|533.869995|     0|\n",
      "|  NYA|1966-01-11|534.289978|534.289978|534.289978|534.289978|534.289978|     0|\n",
      "|  NYA|1966-01-12|533.340027|533.340027|533.340027|533.340027|533.340027|     0|\n",
      "|  NYA|1966-01-13|534.400024|534.400024|534.400024|534.400024|534.400024|     0|\n",
      "|  NYA|1966-01-14|535.450012|535.450012|535.450012|535.450012|535.450012|     0|\n",
      "|  NYA|1966-01-17|537.460022|537.460022|537.460022|537.460022|537.460022|     0|\n",
      "|  NYA|1966-01-18|538.940002|538.940002|538.940002|538.940002|538.940002|     0|\n",
      "|  NYA|1966-01-19|537.669983|537.669983|537.669983|537.669983|537.669983|     0|\n",
      "|  NYA|1966-01-20|535.669983|535.669983|535.669983|535.669983|535.669983|     0|\n",
      "|  NYA|1966-01-21| 535.97998| 535.97998| 535.97998| 535.97998| 535.97998|     0|\n",
      "|  NYA|1966-01-24|537.669983|537.669983|537.669983|537.669983|537.669983|     0|\n",
      "|  NYA|1966-01-25|538.099976|538.099976|538.099976|538.099976|538.099976|     0|\n",
      "|  NYA|1966-01-26|537.570007|537.570007|537.570007|537.570007|537.570007|     0|\n",
      "|  NYA|1966-01-27|537.359985|537.359985|537.359985|537.359985|537.359985|     0|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, trim, coalesce, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StockPred\").getOrCreate()\n",
    "\n",
    "df_clean = spark.read.option(\"header\", True).csv(\"D:\\\\stock\\\\Market.csv\")\n",
    "\n",
    "\n",
    "# Remove extra spaces from Date column\n",
    "df_clean = df_clean.withColumn(\"Date\", trim(col(\"Date\")))\n",
    "\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"Date\",\n",
    "    coalesce(\n",
    "        to_date(col(\"Date\"), \"M/d/yyyy\"),\n",
    "        to_date(col(\"Date\"), \"MM/dd/yyyy\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_clean.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdd1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 112457\n",
      "Number of columns: 8\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|Index|      Date|      Open|      High|       Low|     Close| Adj Close|Volume|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|  NYA|12/31/1965|528.690002|528.690002|528.690002|528.690002|528.690002|     0|\n",
      "|  NYA|  1/3/1966|527.210022|527.210022|527.210022|527.210022|527.210022|     0|\n",
      "|  NYA|  1/4/1966|527.840027|527.840027|527.840027|527.840027|527.840027|     0|\n",
      "|  NYA|  1/5/1966|531.119995|531.119995|531.119995|531.119995|531.119995|     0|\n",
      "|  NYA|  1/6/1966|532.070007|532.070007|532.070007|532.070007|532.070007|     0|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(r\"D:\\stock\\Market.csv\", header=True)\n",
    "\n",
    "print(\"Number of rows:\", df.count())\n",
    "print(\"Number of columns:\", len(df.columns))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf726a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+------+\n",
      "|      Open|      High|       Low|     Close|Volume|\n",
      "+----------+----------+----------+----------+------+\n",
      "|528.690002|528.690002|528.690002|528.690002|     0|\n",
      "|527.210022|527.210022|527.210022|527.210022|     0|\n",
      "|527.840027|527.840027|527.840027|527.840027|     0|\n",
      "|531.119995|531.119995|531.119995|531.119995|     0|\n",
      "|532.070007|532.070007|532.070007|532.070007|     0|\n",
      "+----------+----------+----------+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "sample_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "df_small = df.select(sample_cols)\n",
    "df_small.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0da3104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+------+\n",
      "|      Open|      High|       Low|     Close|Volume|\n",
      "+----------+----------+----------+----------+------+\n",
      "|528.690002|528.690002|528.690002|528.690002|   0.0|\n",
      "|527.210022|527.210022|527.210022|527.210022|   0.0|\n",
      "|527.840027|527.840027|527.840027|527.840027|   0.0|\n",
      "|531.119995|531.119995|531.119995|531.119995|   0.0|\n",
      "|532.070007|532.070007|532.070007|532.070007|   0.0|\n",
      "+----------+----------+----------+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_small = df_small.select(\n",
    "    [col(c).cast(\"double\").alias(c) for c in sample_cols]\n",
    ")\n",
    "df_small.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe7a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+------+\n",
      "|      open|      high|       low|     close|volume|\n",
      "+----------+----------+----------+----------+------+\n",
      "|528.690002|528.690002|528.690002|528.690002|   0.0|\n",
      "|527.210022|527.210022|527.210022|527.210022|   0.0|\n",
      "|527.840027|527.840027|527.840027|527.840027|   0.0|\n",
      "|531.119995|531.119995|531.119995|531.119995|   0.0|\n",
      "|532.070007|532.070007|532.070007|532.070007|   0.0|\n",
      "+----------+----------+----------+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select the real column names and rename them\n",
    "df_clean = df.select(\n",
    "    col(\"Open\").cast(\"double\").alias(\"open\"),\n",
    "    col(\"High\").cast(\"double\").alias(\"high\"),\n",
    "    col(\"Low\").cast(\"double\").alias(\"low\"),\n",
    "    col(\"Close\").cast(\"double\").alias(\"close\"),\n",
    "    col(\"Volume\").cast(\"double\").alias(\"volume\")\n",
    ")\n",
    "\n",
    "df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32a9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|Index|      Date|      Open|      High|       Low|     Close| Adj Close|Volume|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|  NYA|12/31/1965|528.690002|528.690002|528.690002|528.690002|528.690002|     0|\n",
      "|  NYA|  1/3/1966|527.210022|527.210022|527.210022|527.210022|527.210022|     0|\n",
      "|  NYA|  1/4/1966|527.840027|527.840027|527.840027|527.840027|527.840027|     0|\n",
      "|  NYA|  1/5/1966|531.119995|531.119995|531.119995|531.119995|531.119995|     0|\n",
      "|  NYA|  1/6/1966|532.070007|532.070007|532.070007|532.070007|532.070007|     0|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, col\n",
    "\n",
    "# Drop rows where all are null\n",
    "df_clean = spark.read.option(\"header\", True) \\\n",
    "    .option(\"nullValue\", \"null\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"D:/stock/Market.csv\")\n",
    "\n",
    "df_clean = df_clean.na.drop(how=\"all\")\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = [f.name for f in df_clean.schema.fields if f.dataType.simpleString() in (\"int\", \"bigint\", \"double\", \"float\")]\n",
    "\n",
    "# Build impute dictionary only for numeric columns\n",
    "impute_dict = {}\n",
    "for c in numeric_cols:\n",
    "    mean_val = df_clean.select(mean(col(c))).collect()[0][0]\n",
    "    impute_dict[c] = mean_val if mean_val is not None else 0\n",
    "\n",
    "# Fill missing values\n",
    "df_clean = df_clean.na.fill(impute_dict)\n",
    "\n",
    "df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731b1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_features = assembler.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edad2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+----------+----------+----------+----------+------+--------------------+--------------------+\n",
      "|Index|      Date|      Open|      High|       Low|     Close| Adj Close|Volume|            features|     scaled_features|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+--------------------+--------------------+\n",
      "|  NYA|12/31/1965|528.690002|528.690002|528.690002|528.690002|528.690002|     0|[528.690002,528.6...|[-0.7990700677731...|\n",
      "|  NYA|  1/3/1966|527.210022|527.210022|527.210022|527.210022|527.210022|     0|[527.210022,527.2...|[-0.7992359343876...|\n",
      "|  NYA|  1/4/1966|527.840027|527.840027|527.840027|527.840027|527.840027|     0|[527.840027,527.8...|[-0.7991653274899...|\n",
      "|  NYA|  1/5/1966|531.119995|531.119995|531.119995|531.119995|531.119995|     0|[531.119995,531.1...|[-0.7987977298279...|\n",
      "|  NYA|  1/6/1966|532.070007|532.070007|532.070007|532.070007|532.070007|     0|[532.070007,532.0...|[-0.7986912586092...|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_scaled = scaler_model.transform(df_features)\n",
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "022e2f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc0c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df_scaled.select(\"scaled_features\", \"close\").toPandas()\n",
    "pdf = pdf.rename(columns={\"close\": \"TARGET\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "054f74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, spark_df):\n",
    "        data = spark_df.collect()\n",
    "\n",
    "        # Convert list of numpy arrays into one big NumPy array\n",
    "        features = np.array([row.scaled_features.toArray() for row in data], dtype=np.float32)\n",
    "        targets = np.array([row.close for row in data], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "        self.X = torch.from_numpy(features)\n",
    "        self.y = torch.from_numpy(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbc2884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\miniconda3\\envs\\stock\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "621cdfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcda20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect from Spark DataFrame\n",
    "data = df_scaled.select(\"scaled_features\", \"close\").collect()\n",
    "\n",
    "X = np.array([row.scaled_features.toArray() for row in data], dtype=np.float32)\n",
    "y = np.array([row.close for row in data], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63fcbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a98c2b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert only if they are NumPy arrays\n",
    "if isinstance(X_train, np.ndarray):\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = torch.from_numpy(y_train).float().view(-1, 1)\n",
    "if isinstance(X_test, np.ndarray):\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "if isinstance(y_test, np.ndarray):\n",
    "    y_test = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "# Wrap into TensorDataset\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd3eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyAdam:\n",
    "    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.params = list(params)          # model parameters\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]  # first moment\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]  # second moment\n",
    "        self.t = 0  # time step\n",
    "\n",
    "    def step(self): #track of opt steps\n",
    "        self.t += 1\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            g = p.grad.data\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g * g)\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameter\n",
    "            p.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12635f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class StockNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(StockNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # regression (TARGET = price)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize model\n",
    "model = StockNN(input_dim=X_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use custom Adam instead of torch.optim.Adam\n",
    "optimizer = MyAdam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b35ddf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2497"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c5cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29fa3ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions: [ 2861.205     692.44086  2341.0344   7636.1357  12862.609  ]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test).numpy()\n",
    "\n",
    "print(\"Sample predictions:\", preds[:5].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "223f2eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 768339.3986, Test Loss: 16385.5905, Train RMSE: 876.5497, Test RMSE: 128.0062\n",
      "Epoch 2, Train Loss: 652077.2239, Test Loss: 20664.3792, Train RMSE: 807.5130, Test RMSE: 143.7511\n",
      "Epoch 3, Train Loss: 648604.1198, Test Loss: 5513.8133, Train RMSE: 805.3596, Test RMSE: 74.2551\n",
      "Epoch 4, Train Loss: 625072.3534, Test Loss: 7867.3009, Train RMSE: 790.6152, Test RMSE: 88.6978\n",
      "Epoch 5, Train Loss: 616930.9409, Test Loss: 34534.8973, Train RMSE: 785.4495, Test RMSE: 185.8357\n",
      "Epoch 6, Train Loss: 618155.3060, Test Loss: 8093.1736, Train RMSE: 786.2285, Test RMSE: 89.9621\n",
      "Epoch 7, Train Loss: 604505.5840, Test Loss: 10917.7047, Train RMSE: 777.4996, Test RMSE: 104.4878\n",
      "Epoch 8, Train Loss: 616733.4181, Test Loss: 4881.8570, Train RMSE: 785.3238, Test RMSE: 69.8703\n",
      "Epoch 9, Train Loss: 606415.4309, Test Loss: 9303.1683, Train RMSE: 778.7268, Test RMSE: 96.4529\n",
      "Epoch 10, Train Loss: 593864.0790, Test Loss: 42571.6130, Train RMSE: 770.6258, Test RMSE: 206.3289\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# assume: model, criterion, optimizer, train_dl, test_dl are already defined\n",
    "\n",
    "for epoch in range(10):  # number of epochs\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for xb, yb in train_dl:  # batches from DataLoader\n",
    "        preds = model(xb)                 # forward pass\n",
    "        loss = criterion(preds, yb)       # compute loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()             # reset gradients\n",
    "        loss.backward()                   # backward pass\n",
    "        optimizer.step()                  # update params\n",
    "\n",
    "    # --- Compute train loss & RMSE ---\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_rmse = np.sqrt(train_loss)\n",
    "\n",
    "    # --- Evaluation on test data ---\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_dl:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_losses)\n",
    "    test_rmse = np.sqrt(test_loss)\n",
    "\n",
    "    # --- Print results ---\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "        f\"Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fabf0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3734939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0e2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804f8170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Cleaning:\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|Index|Date      |Open      |High      |Low       |Close     |Adj Close |Volume|\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "|NYA  |1965-12-31|528.690002|528.690002|528.690002|528.690002|528.690002|0     |\n",
      "|NYA  |1966-01-03|527.210022|527.210022|527.210022|527.210022|527.210022|0     |\n",
      "|NYA  |1966-01-04|527.840027|527.840027|527.840027|527.840027|527.840027|0     |\n",
      "|NYA  |1966-01-05|531.119995|531.119995|531.119995|531.119995|531.119995|0     |\n",
      "|NYA  |1966-01-06|532.070007|532.070007|532.070007|532.070007|532.070007|0     |\n",
      "+-----+----------+----------+----------+----------+----------+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, col, coalesce, to_date\n",
    "\n",
    "# 1. Create Spark session\n",
    "spark = SparkSession.builder.appName(\"MovieRecSys\").getOrCreate()\n",
    "\n",
    "# 2. Load CSV into df  (⚠️ update path)\n",
    "df = spark.read.option(\"header\", True).csv(r\"D:\\stock\\Market.csv\")\n",
    "\n",
    "# 3. Trim spaces in Date column\n",
    "df_clean = df.withColumn(\"Date\", trim(col(\"Date\")))\n",
    "\n",
    "# 4. Try parsing both formats: \"M/d/yyyy\" and \"MM/dd/yyyy\"\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"Date\",\n",
    "    coalesce(\n",
    "        to_date(col(\"Date\"), \"M/d/yyyy\"),\n",
    "        to_date(col(\"Date\"), \"MM/dd/yyyy\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Drop rows where Date could not be parsed\n",
    "df_clean = df_clean.na.drop(subset=[\"Date\"])\n",
    "\n",
    "print(\"After Cleaning:\")\n",
    "df_clean.show(5, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "stock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
